{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YCGOutreach.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN20GqDfjmnrIN3r1xFGNH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintualkassoum/bintualkassoum/blob/main/YCGOutreach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slDmZAfcwSoK"
      },
      "outputs": [],
      "source": [
        "#Import Packages \n",
        "\n",
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Websites \n",
        "def find_jobs_from(website, job_title, location, job_type, desired_characs, filename=\"results.xls\"):      \n",
        "    \n",
        "    if website == 'Indeed':\n",
        "        job_soup = load_indeed_jobs_div(job_title, location)\n",
        "        jobs_list, num_listings = extract_job_information_indeed(job_soup, desired_characs)\n",
        "    \n",
        "    if website == 'CharityVillage':\n",
        "        location_of_driver = \"/Users/bintualkassoum/Downloads/chromedriver\"\n",
        "        driver = initiate_driver(location_of_driver, browser='chrome')\n",
        "        job_soup = make_job_search(job_title, location, driver)\n",
        "        jobs_list, num_listings = extract_job_information_charityvillagejobs(job_soup, desired_characs)\n",
        "\n",
        "    \n",
        "    save_jobs_to_excel(jobs_list, filename)\n",
        " \n",
        "    print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings, \n",
        "                                                                          website, filename))"
      ],
      "metadata": {
        "id": "a9JusIKrweKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Report \n",
        "def save_jobs_to_excel(jobs_list, filename):\n",
        "    jobs = pd.DataFrame(jobs_list)\n",
        "    jobs.to_excel(filename)\n",
        " \n",
        "    print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings, \n",
        "                                                                          website, filename))"
      ],
      "metadata": {
        "id": "sMNtdzJRx1Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Indeed Functions \n",
        "def load_indeed_jobs_div(job_title, location):\n",
        "    getVars = {'q' : job_title, 'l' : location, 'jt' : job_type, 'fromage' : 'last', 'sort' : 'date'}\n",
        "    url = ('https://ca.indeed.com' + urllib.parse.urlencode(getVars))\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    job_soup = soup.find(id=\"resultsCol\")\n",
        "    return job_soup\n",
        "\n",
        "def extract_job_information_indeed(job_soup, desired_characs):\n",
        "    job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "     \n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "    \n",
        "    \n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_indeed(job_elem))\n",
        "        extracted_info.append(titles)                    \n",
        "    \n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_indeed(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "    \n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_indeed(job_elem))\n",
        "        extracted_info.append(links)\n",
        "    \n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_indeed(job_elem))\n",
        "        extracted_info.append(dates)\n",
        "    \n",
        "    jobs_list = {}\n",
        "    \n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "    \n",
        "    num_listings = len(extracted_info[0])\n",
        "    \n",
        "    return jobs_list, num_listings\n",
        "\n",
        "\n",
        "def extract_job_title_indeed(job_elem):\n",
        "    title_elem = job_elem.find('h2', class_='title')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        "\n",
        "def extract_company_indeed(job_elem):\n",
        "    company_elem = job_elem.find('span', class_='company')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_indeed(job_elem):\n",
        "    link = job_elem.find('a')['href']\n",
        "    link = 'https://ca.indeed.com' + link\n",
        "    return link\n",
        "\n",
        "def extract_date_indeed(job_elem):\n",
        "    date_elem = job_elem.find('span', class_='date')\n",
        "    date = date_elem.text.strip()\n",
        "    return date\n"
      ],
      "metadata": {
        "id": "-i1cKDgpyYd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Charity Village Functions\n",
        "def initiate_driver(location_of_driver, browser):\n",
        "    if browser == 'chrome':\n",
        "        driver = webdriver.Chrome(executable_path=(location_of_driver + \"/chromedriver\"))\n",
        "    elif browser == 'firefox':\n",
        "        driver = webdriver.Firefox(executable_path=(location_of_driver + \"/firefoxdriver\"))\n",
        "    elif browser == 'safari':\n",
        "        driver = webdriver.Safari(executable_path=(location_of_driver + \"/safaridriver\"))\n",
        "    elif browser == 'edge':\n",
        "        driver = webdriver.Edge(executable_path=(location_of_driver + \"/edgedriver\"))\n",
        "    return driver\n",
        "\n",
        "def make_job_search(job_title, location, driver):\n",
        "    driver.get('https://charityvillage.com/?ts=1640639872')\n",
        "    \n",
        "    # Select the job box\n",
        "    job_title_box = driver.find_element_by_name('Search')\n",
        "\n",
        "    # Send job information\n",
        "    job_title_box.send_keys(job_title)\n",
        "\n",
        "    # Selection location box\n",
        "    location_box = driver.find_element_by_id('job-search-location')\n",
        "    \n",
        "    # Send location information\n",
        "    location_box.send_keys(location)\n",
        "    \n",
        "    # Find Search button\n",
        "    search_button = driver.find_element_by_id('searchsubmit')\n",
        "    search_button.click()\n",
        "\n",
        "    driver.implicitly_wait(5)\n",
        "\n",
        "    page_source = driver.page_source\n",
        "    \n",
        "    job_soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "    \n",
        "    return job_soup\n",
        "\n",
        "\n",
        "def extract_job_information_charityvillagejobs(job_soup, desired_characs):\n",
        "    \n",
        "    job_elems = job_soup.find_all('div', class_=\"cl-job\")\n",
        "     \n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "    \n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_charityvillagejobs(job_elem))\n",
        "        extracted_info.append(titles) \n",
        "                           \n",
        "    \n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_cwjobs(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "    \n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_cwjobs(job_elem))\n",
        "        extracted_info.append(links)\n",
        "                \n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_cwjobs(job_elem))\n",
        "        extracted_info.append(dates)    \n",
        "    \n",
        "    jobs_list = {}\n",
        "    \n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "    \n",
        "    num_listings = len(extracted_info[0])\n",
        "    \n",
        "    return jobs_list, num_listings\n",
        "\n",
        "\n",
        "def extract_job_title_cwjobs(job_elem):\n",
        "    title_elem = job_elem.find('h2')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        " \n",
        "def extract_company_cwjobs(job_elem):\n",
        "    company_elem = job_elem.find('h3')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_cwjobs(job_elem):\n",
        "    link = job_elem.find('a')['href']\n",
        "    return link\n",
        "\n",
        "def extract_date_cwjobs(job_elem):\n",
        "    link_elem = job_elem.find('li', class_='date-posted')\n",
        "    link = link_elem.text.strip()\n",
        "    return link"
      ],
      "metadata": {
        "id": "Td5XK-If0jQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}